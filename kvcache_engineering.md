# KV Cache å·¥ç¨‹å®ç°æŒ‡å—

## 1. TensorFlow ä¸­çš„ KV Cache å®ç°

### **TensorFlow çš„æŒ‘æˆ˜**

```
TensorFlow çš„ç‰¹ç‚¹ï¼š
âœ… ä¼˜ç‚¹ï¼š
   - Graph æ¨¡å¼ç¼–è¯‘ä¼˜åŒ–
   - é™æ€å½¢çŠ¶æ¨å¯¼
   - åˆ†å¸ƒå¼æ”¯æŒå®Œå–„

âŒ å›°éš¾ï¼š
   - æ¨ç†æ—¶åºåˆ—é•¿åº¦åŠ¨æ€å˜åŒ–ï¼ˆGraph éš¾ä»¥ä¼˜åŒ–ï¼‰
   - KV Cache éœ€è¦åŠ¨æ€æ›´æ–°ï¼ˆä¸ Graph çš„é™æ€ç‰¹æ€§å†²çªï¼‰
   - Eager æ‰§è¡Œè™½ç„¶çµæ´»ä½†æ— æ³•å……åˆ†ä¼˜åŒ–
```

### **æ–¹æ¡ˆ 1ï¼šEager Executionï¼ˆæ¨èï¼‰**

```python
# è¿™æ˜¯ TensorFlow ä¸­æœ€å®ç”¨çš„æ–¹æ¡ˆ

import tensorflow as tf

class KVCacheTransformerLayer(tf.keras.layers.Layer):
    def __init__(self, hidden_dim, num_heads):
        super().__init__()
        self.hidden_dim = hidden_dim
        
    def call(self, x, kv_cache=None, training=False):
        # Eager æ‰§è¡Œï¼ŒåŠ¨æ€å¤„ç† KV Cache
        
        if training:
            # è®­ç»ƒæ—¶ï¼Œä¸ä½¿ç”¨ KV Cacheï¼ˆå®Œæ•´åºåˆ—ï¼‰
            Q, K, V = self.compute_qkv(x)
            output = self.attention(Q, K, V)
            return output, None
        
        else:
            # æ¨ç†æ—¶ï¼Œä½¿ç”¨ KV Cache
            new_token = x  # shape: (1, hidden_dim)
            Q_new = self.W_q(new_token)
            K_new = self.W_k(new_token)
            V_new = self.W_v(new_token)
            
            if kv_cache is None:
                # ç¬¬ä¸€ä¸ª token
                K_full = K_new
                V_full = V_new
                kv_cache = (K_new, V_new)
            else:
                # åç»­ token
                K_cache, V_cache = kv_cache
                K_full = tf.concat([K_cache, K_new], axis=0)  # (seq_len, dim)
                V_full = tf.concat([V_cache, V_new], axis=0)
                kv_cache = (K_full, V_full)
            
            output = self.attention(Q_new, K_full, V_full)
            return output, kv_cache

# æ¨ç†å¾ªç¯
@tf.function(jit_compile=True)  # å•æ­¥ä¼˜åŒ–
def inference_step(model, input_token, kv_caches):
    outputs = []
    new_kv_caches = []
    
    x = input_token
    for i, layer in enumerate(model.layers):
        x, new_cache = layer(x, kv_cache=kv_caches[i])
        new_kv_caches.append(new_cache)
        outputs.append(x)
    
    return x, new_kv_caches

# ç”Ÿæˆå¾ªç¯
def generate(model, prompt, max_len):
    kv_caches = [None] * len(model.layers)
    tokens = prompt
    
    for step in range(max_len):
        last_token = tf.expand_dims(tokens[-1:], 0)
        
        # æ‰§è¡Œä¸€æ­¥æ¨ç†
        logits, kv_caches = inference_step(
            model, last_token, kv_caches
        )
        
        next_token = tf.argmax(logits, axis=-1)
        tokens = tf.concat([tokens, next_token], axis=0)
    
    return tokens
```

### **æ–¹æ¡ˆ 2ï¼šä½¿ç”¨ tf.RaggedTensorï¼ˆåŠ¨æ€å½¢çŠ¶ï¼‰**

```python
# å¯¹äºéœ€è¦æ›´å¥½å›¾ä¼˜åŒ–çš„åœºæ™¯

import tensorflow as tf

class DynamicKVCache:
    def __init__(self, max_seq_len, hidden_dim, dtype=tf.float32):
        # ä½¿ç”¨ TensorVariable å­˜å‚¨ KV Cache
        self.k_cache = tf.Variable(
            tf.zeros((max_seq_len, hidden_dim), dtype=dtype),
            trainable=False,
            name='k_cache'
        )
        self.v_cache = tf.Variable(
            tf.zeros((max_seq_len, hidden_dim), dtype=dtype),
            trainable=False,
            name='v_cache'
        )
        self.length = tf.Variable(0, trainable=False, dtype=tf.int32)
    
    def update(self, k_new, v_new):
        # åŸå­æ“ä½œï¼šæ›´æ–°ç¼“å­˜å¹¶å¢åŠ é•¿åº¦
        idx = self.length
        new_len = idx + tf.shape(k_new)[0]
        
        # ä½¿ç”¨ assign æ“ä½œ
        self.k_cache[idx:new_len].assign(k_new)
        self.v_cache[idx:new_len].assign(v_new)
        self.length.assign(new_len)
    
    def get_full(self):
        length = self.length
        return (
            self.k_cache[:length],
            self.v_cache[:length]
        )
    
    def reset(self):
        self.length.assign(0)

# ä½¿ç”¨ç¤ºä¾‹
@tf.function
def attention_with_cache(Q, k_cache, v_cache):
    K_full, V_full = k_cache.get_full()
    
    # è®¡ç®— Attention
    scores = tf.matmul(Q, K_full, transpose_b=True)
    scores = scores / tf.math.sqrt(tf.cast(tf.shape(K_full)[-1], tf.float32))
    weights = tf.nn.softmax(scores, axis=-1)
    output = tf.matmul(weights, V_full)
    
    return output
```

### **æ–¹æ¡ˆ 3ï¼šè‡ªå®šä¹‰ Opï¼ˆé«˜æ€§èƒ½ï¼‰**

```python
# éœ€è¦ç¼–å†™ CUDA/C++ ä»£ç 

import tensorflow as tf

# è‡ªå®šä¹‰ opï¼Œç›´æ¥åœ¨ GPU ä¸Šæ“ä½œ
@tf.function
def fused_attention_with_kv_cache(
    Q, K_cache, V_cache, K_new, V_new
):
    """
    èåˆæ“ä½œï¼š
    1. æ‹¼æ¥ K, V
    2. è®¡ç®— Attention
    3. æ›´æ–°ç¼“å­˜
    
    å®Œå…¨åœ¨ GPU ä¸Šæ‰§è¡Œï¼Œæ— ä¸­é—´æ•°æ®äº¤æ¢
    """
    # è°ƒç”¨è‡ªå®šä¹‰ CUDA op
    output, new_k_cache, new_v_cache = \
        tf.raw_ops.FusedAttentionWithKVCache(
            Q=Q,
            K_cache=K_cache,
            V_cache=V_cache,
            K_new=K_new,
            V_new=V_new,
            # å…¶ä»–å‚æ•°...
        )
    
    return output, new_k_cache, new_v_cache
```

## 2. TensorFlow Graph ä¿®æ”¹ç­–ç•¥

### **ä¼ ç»Ÿ Graphï¼ˆæ—  KV Cacheï¼‰**

```
Graph ç»“æ„ï¼š

Input â”€â”€â†’ Embedding â”€â”€â†’ Layer_0 â”€â”€â†’ Layer_1 â”€â”€â†’ ... â”€â”€â†’ Output
                           â†“          â†“
                        Attention  Attention
                           â†“          â†“
                        å®Œæ•´åºåˆ—   å®Œæ•´åºåˆ—
```

### **æ”¹é€ åçš„ Graphï¼ˆKV Cacheï¼‰**

#### **æ–¹æ¡ˆ Aï¼šåŠ¨æ€ Graphï¼ˆä¸ä¿®æ”¹é™æ€å›¾ç»“æ„ï¼‰**

```python
# ä½¿ç”¨ tf.cond æˆ– tf.while_loop åŠ¨æ€å¤„ç†

def build_inference_graph():
    @tf.function
    def step_fn(token_idx, kv_caches):
        # åœ¨ Graph å†…éƒ¨åŠ¨æ€æ‰§è¡Œ
        
        # å¯¹å½“å‰ token ç¼–ç 
        x = embed(token_idx)
        
        new_kv_caches = []
        for i, layer in enumerate(layers):
            if kv_caches[i] is None:
                # é¦–æ¬¡ï¼Œè®¡ç®—å®Œæ•´ Attention
                x = layer.full_attention(x)
                new_kv_cache = (K, V)
            else:
                # åç»­ï¼Œä½¿ç”¨ç¼“å­˜
                x = layer.incremental_attention(x, kv_caches[i])
                new_kv_cache = update_cache(kv_caches[i], K_new, V_new)
            
            new_kv_caches.append(new_kv_cache)
        
        return x, new_kv_caches
    
    return step_fn

# è°ƒç”¨
step_fn = build_inference_graph()
kv_caches = [None] * num_layers

for i in range(seq_len):
    output, kv_caches = step_fn(tokens[i], kv_caches)
```

#### **æ–¹æ¡ˆ Bï¼šæ˜¾å¼åˆ†æ”¯ Graphï¼ˆä¿®æ”¹å›¾ç»“æ„ï¼‰**

```python
# ä¸ºè®­ç»ƒå’Œæ¨ç†åˆ†åˆ«æ„å»ºä¸åŒçš„ Graph

class DualGraphModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.layers_train = [...]  # è®­ç»ƒç”¨ï¼šå¤„ç†å®Œæ•´åºåˆ—
        self.layers_infer = [...]  # æ¨ç†ç”¨ï¼šå¢é‡è®¡ç®—
    
    @tf.function(input_signature=[...])
    def call_train(self, input_ids):
        # è®­ç»ƒ Graphï¼šæ ‡å‡† Transformer
        x = embedding(input_ids)
        for layer in self.layers_train:
            x = layer(x)
        return x
    
    @tf.function
    def call_infer(self, token_id, kv_caches):
        # æ¨ç† Graphï¼šä½¿ç”¨ KV Cache
        x = embedding(token_id)
        new_kv_caches = []
        
        for i, layer in enumerate(self.layers_infer):
            if kv_caches[i] is None:
                x = layer.forward(x)
                new_cache = None
            else:
                x = layer.forward_incremental(x, kv_caches[i])
                new_cache = layer.updated_cache
            
            new_kv_caches.append(new_cache)
        
        return x, new_kv_caches
    
    def call(self, input_ids, training=True):
        if training:
            return self.call_train(input_ids)
        else:
            # éœ€è¦å¤–éƒ¨å¾ªç¯è°ƒç”¨ call_infer
            return None  # åœ¨ Python å¾ªç¯ä¸­è°ƒç”¨

# ä½¿ç”¨
model = DualGraphModel()

# è®­ç»ƒæ—¶
loss = model(input_ids, training=True)

# æ¨ç†æ—¶ï¼ˆPython å¾ªç¯ï¼‰
kv_caches = [None] * num_layers
for token in sequence:
    output, kv_caches = model.call_infer(token, kv_caches)
```

#### **æ–¹æ¡ˆ Cï¼šä½¿ç”¨ tf.while_loopï¼ˆæœ€ä¼˜åŒ–ï¼‰**

```python
@tf.function
def generate_with_while_loop(prompt_ids, max_steps):
    # è¿™æ ·èƒ½å……åˆ†åˆ©ç”¨ Graph ä¼˜åŒ–
    
    def body_fn(step, token_id, kv_caches, output_ids):
        # å•æ­¥æ¨ç†
        x = embedding(token_id)
        new_kv_caches = []
        
        for i, layer in enumerate(layers):
            x, new_cache = layer.incremental_forward(x, kv_caches[i])
            new_kv_caches.append(new_cache)
        
        next_token_id = tf.argmax(x, axis=-1)
        output_ids = tf.concat([output_ids, [next_token_id]], axis=0)
        
        return step + 1, next_token_id, new_kv_caches, output_ids
    
    def cond_fn(step, *args):
        return step < max_steps
    
    kv_caches = [None] * num_layers
    initial_token = prompt_ids[-1]
    
    _, _, _, final_ids = tf.while_loop(
        cond_fn,
        body_fn,
        loop_vars=[
            tf.constant(0),
            initial_token,
            kv_caches,
            prompt_ids
        ]
    )
    
    return final_ids
```

## 3. PyTorch vs TensorFlow å¯¹æ¯”

### **PyTorchï¼ˆå®é™…æ›´æ˜“ç”¨ï¼‰**

```python
# PyTorch çš„ Eager æ‰§è¡Œå¤©ç„¶é€‚åˆ KV Cache

class TransformerLayer(nn.Module):
    def forward(self, x, kv_cache=None):
        # åŠ¨æ€å¤„ç†ï¼Œæ— éœ€ Graph ä¿®æ”¹
        Q, K, V = self.compute_qkv(x)
        
        if kv_cache is not None:
            K_cache, V_cache = kv_cache
            K = torch.cat([K_cache, K], dim=0)
            V = torch.cat([V_cache, V], dim=0)
        
        output = self.attention(Q, K, V)
        
        return output, (K, V)

# æ¨ç†å¾ªç¯éå¸¸è‡ªç„¶
def generate(model, prompt):
    kv_caches = [None] * len(model.layers)
    
    for _ in range(max_len):
        x = prompt[-1:]
        for i, layer in enumerate(model.layers):
            x, kv_caches[i] = layer(x, kv_caches[i])
        prompt = torch.cat([prompt, x], dim=0)
    
    return prompt
```

### **å¯¹æ¯”è¡¨**

| æ–¹é¢ | PyTorch | TensorFlow |
|------|---------|----------|
| **KV Cache æ˜“ç”¨æ€§** | â­â­â­â­â­ | â­â­ |
| **Graph ä¿®æ”¹éœ€æ±‚** | æ—  | éœ€è¦ï¼ˆEager é™¤å¤–ï¼‰ |
| **æ¨ç†å¾ªç¯** | ä»£ç ç®€æ´ | ç›¸å¯¹å¤æ‚ |
| **æ€§èƒ½** | æœ€ä¼˜ | å¯æ¥å— |
| **å­¦ä¹ æˆæœ¬** | ä½ | é«˜ |
| **ç”Ÿäº§éƒ¨ç½²** | vLLM âœ… | å¯è¡Œä½†è¾ƒå¤æ‚ |

## 4. å®ç°å»ºè®®ï¼ˆLONGER é¡¹ç›®ï¼‰

### **åˆ†é˜¶æ®µå»ºè®®**

```
1ï¸âƒ£ åŸå‹å¼€å‘ â†’ PyTorch
   â”œâ”€ å¿«é€Ÿè¿­ä»£ KV Cache ä¼˜åŒ–é€»è¾‘
   â”œâ”€ è°ƒè¯•å’ŒéªŒè¯ User/Item ç²’åº¦å¤ç”¨
   â””â”€ æ— éœ€ä¿®æ”¹æ¡†æ¶ä»£ç 

2ï¸âƒ£ å­¦æœ¯å‘è¡¨ â†’ PyTorch
   â””â”€ å¼€æºç¤¾åŒºä¸»æµï¼Œæ˜“è·å¾—å…³æ³¨

3ï¸âƒ£ ç”Ÿäº§éƒ¨ç½²
   
   é€‰é¡¹ Aï¼šPyTorch + vLLMï¼ˆæ¨èï¼‰
   â”œâ”€ åŸç”Ÿ KV Cache ç®¡ç†
   â”œâ”€ ç‰©ç†å—æ˜ å°„
   â”œâ”€ PagedAttention ä¼˜åŒ–
   â””â”€ ä¸šç•Œæ ‡å‡†
   
   é€‰é¡¹ Bï¼šTensorFlow + Eager Execution
   â”œâ”€ æ— éœ€ä¿®æ”¹ Graph
   â”œâ”€ ä»£ç é€»è¾‘æ¸…æ™°
   â”œâ”€ æ€§èƒ½ -10-20%ï¼ˆå¯æ¥å—ï¼‰
   â””â”€ é€‚åˆå·²æœ‰ TF åŸºç¡€è®¾æ–½çš„å›¢é˜Ÿ
   
   é€‰é¡¹ Cï¼šè‡ªå®šä¹‰ CUDA Op
   â”œâ”€ æœ€é«˜æ€§èƒ½
   â”œâ”€ ç»´æŠ¤æˆæœ¬é«˜
   â””â”€ ä»…åœ¨å¿…è¦æ—¶è€ƒè™‘
```

### **æ ¸å¿ƒå·¥ç¨‹é—®é¢˜æ¸…å•**

```
[ ] KV Cache çš„åˆå§‹å¤§å°å¦‚ä½•è®¾å®šï¼Ÿ
    â†’ åŠ¨æ€æ‰©å±•è¿˜æ˜¯å›ºå®šä¸Šé™ï¼Ÿ

[ ] å¤šè¯·æ±‚ä¸‹ KV Cache å¦‚ä½•å…±äº«ï¼Ÿ
    â†’ ç‰©ç†å—æ˜ å°„è¿˜æ˜¯ç®€å•å¤åˆ¶ï¼Ÿ

[ ] æ˜¾å­˜ä¸è¶³æ—¶çš„ fallback ç­–ç•¥ï¼Ÿ
    â†’ CPU å†…å­˜ / SSD é¢„å–ï¼Ÿ

[ ] å¦‚ä½•å¤„ç† batch ä¸­åºåˆ—é•¿åº¦ä¸ä¸€è‡´ï¼Ÿ
    â†’ Padding è¿˜æ˜¯ ragged tensorï¼Ÿ

[ ] KV Cache é¢„çƒ­å’Œé¢„åŠ è½½ï¼Ÿ
    â†’ å¯¹æ€§èƒ½çš„å½±å“æœ‰å¤šå¤§ï¼Ÿ

[ ] ä¸é‡åŒ–ï¼ˆINT8ï¼‰çš„ç»“åˆï¼Ÿ
    â†’ KV Cache é‡åŒ–ä¼šå½±å“ç²¾åº¦å—ï¼Ÿ

[ ] åˆ†å¸ƒå¼æ¨ç†ä¸­çš„ KV Cache åŒæ­¥ï¼Ÿ
    â†’ å¼ é‡å¹¶è¡Œ / æµæ°´çº¿å¹¶è¡Œä¸‹çš„ç­–ç•¥ï¼Ÿ
```

## 5. æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

```
æ¨ç†ä¼˜åŒ–çš„å…³é”®æŒ‡æ ‡ï¼š

ğŸš€ å»¶è¿Ÿä¼˜åŒ–
  [ ] å•æ­¥æ¨ç†æ—¶é—´ < 100msï¼ˆæ¨èï¼‰
  [ ] KV Cache è®¿é—®æ˜¯å¦æ˜¯ç“¶é¢ˆï¼Ÿ
  [ ] GPU åˆ©ç”¨ç‡æ˜¯å¦å……åˆ†ï¼ˆ>80%ï¼‰ï¼Ÿ

ğŸ’¾ æ˜¾å­˜ä¼˜åŒ–
  [ ] KV Cache å ç”¨ vs æ¨¡å‹å‚æ•°æ¯”
  [ ] å³°å€¼æ˜¾å­˜æ˜¯å¦åœ¨è®¾å¤‡é™åˆ¶å†…ï¼Ÿ
  [ ] æ˜¯å¦å‡ºç°é¢‘ç¹çš„ OOMï¼Ÿ

ğŸ”„ ååä¼˜åŒ–
  [ ] æ‰¹å¤„ç†å¤§å°çš„æœ€ä¼˜å€¼
  [ ] å¹¶å‘è¯·æ±‚æ•°å¯¹ååçš„å½±å“
  [ ] åºåˆ—é•¿åº¦å¢é•¿æ—¶çš„ååä¸‹é™æ›²çº¿

ğŸ¯ å‡†ç¡®ç‡
  [ ] KV Cache ä¼˜åŒ–æ˜¯å¦å½±å“è¾“å‡ºï¼Ÿ
  [ ] é‡åŒ–åçš„ KV Cache ç²¾åº¦
  [ ] ä¸ full precision çš„ diff
```

## å‚è€ƒå®ç°

- **vLLM**ï¼šhttps://github.com/lm-sys/vllm
  - ç‰©ç†å—ç®¡ç†
  - PagedAttention
  - KV Cache æœ€ä½³å®è·µ

- **HuggingFace Transformers**ï¼šhttps://github.com/huggingface/transformers
  - ç®€å• KV Cache å®ç°
  - å¤šæ¡†æ¶æ”¯æŒ

- **TensorFlow Text**ï¼š
  - é¢„å¤„ç†ä¼˜åŒ–
  - TF native KV Cache æ”¯æŒï¼ˆæ–°ç‰ˆæœ¬ï¼‰
