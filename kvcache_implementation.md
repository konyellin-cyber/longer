# KV Cache åŸç†è¯¦è§£

## 1. KV Cache çš„å­˜å‚¨å’Œè¯»å–æµç¨‹

### **ä¼ ç»Ÿæ–¹å¼ï¼ˆæ—  KV Cacheï¼‰**

```
æ¯æ¬¡æ¨ç†ï¼ˆç”Ÿæˆæ–° tokenï¼‰ï¼š

è¾“å…¥åºåˆ—ï¼š[token_0, token_1, token_2, ..., token_n]
           â†“
         embedding å±‚
           â†“
         Transformer å±‚
           â†“
    è®¡ç®—æ‰€æœ‰ä½ç½®çš„ Q, K, V
           â†“
      åšå®Œæ•´ Attention
           â†“
          è¾“å‡º
           
ğŸ”´ é—®é¢˜ï¼šå³ä½¿ token_0 åˆ° token_n æ²¡å˜ï¼Œä¹Ÿè¦å…¨éƒ¨é‡æ–°è®¡ç®— Q, K, V
```

### **KV Cache æ–¹å¼**

```
ç¬¬ä¸€æ¬¡æ¨ç†ï¼ˆç”Ÿæˆ token_n+1ï¼‰ï¼š

è¾“å…¥ï¼š[token_0, token_1, ..., token_n]
      â†“
è®¡ç®— Q, K, V
      â†“
ä¿å­˜ K, V åˆ° KV Cacheï¼ˆå¤–å­˜å‚¨æˆ–æ˜¾å­˜ï¼‰
      â†“
ç”Ÿæˆ token_n+1

ç¬¬äºŒæ¬¡æ¨ç†ï¼ˆç”Ÿæˆ token_n+2ï¼‰ï¼š

è¾“å…¥ï¼šåªæœ‰ token_n+1ï¼ˆæ–° tokenï¼‰
      â†“
è®¡ç®—æ–° Q, K, V
      â†“
ä» KV Cache è¯»å–å†å² K, V
      â†“
æ‹¼æ¥ï¼šK_full = [K_cache, K_new]
      V_full = [V_cache, V_new]
      â†“
åš Attention(Q_new, K_full, V_full)
      â†“
æ›´æ–° KV Cacheï¼Œç”Ÿæˆ token_n+2
```

### **KV Cache çš„å­˜å‚¨ä½ç½®**

æ ¹æ®åºåˆ—é•¿åº¦å’Œç¡¬ä»¶ï¼Œæœ‰ä¸‰ç§å­˜å‚¨æ–¹å¼ï¼š

```
1ï¸âƒ£ æ˜¾å­˜ï¼ˆGPU Memoryï¼‰- æ¨è
   â”œâ”€ ä½ç½®ï¼šGPU æ˜¾å­˜ï¼ˆä¸æ¨¡å‹å‚æ•°åŒä½ç½®ï¼‰
   â”œâ”€ ä¼˜ç‚¹ï¼šè®¿é—®æœ€å¿«ï¼Œå®Œå…¨åˆ©ç”¨ GPU è®¡ç®—èƒ½åŠ›
   â”œâ”€ ç¼ºç‚¹ï¼šæ˜¾å­˜æœ‰é™ï¼Œè¶…é•¿åºåˆ—å®¹æ˜“æº¢å‡º
   â””â”€ ä½¿ç”¨åœºæ™¯ï¼šçŸ­åˆ°ä¸­ç­‰åºåˆ—ï¼ˆ<100k tokensï¼‰
   
   æ˜¾å­˜ä½¿ç”¨é‡è®¡ç®—ï¼š
   KV_memory = 2 Ã— seq_len Ã— hidden_dim Ã— num_layers Ã— batch_size Ã— dtype_size
   
   ç¤ºä¾‹ï¼ˆ1000 tokensï¼‰ï¼š
   = 2 Ã— 1000 Ã— 768 Ã— 50 Ã— 1 Ã— 2bytes
   â‰ˆ 150MB per request

2ï¸âƒ£ CPU å†…å­˜ - ä¸­ç­‰
   â”œâ”€ ä½ç½®ï¼šä¸»æœº RAM
   â”œâ”€ ä¼˜ç‚¹ï¼šå®¹é‡å¤§ï¼Œå¯å­˜å‚¨è¶…é•¿åºåˆ—
   â”œâ”€ ç¼ºç‚¹ï¼šCPU-GPU æ•°æ®ä¼ è¾“å¼€é”€å¤§
   â””â”€ ä½¿ç”¨åœºæ™¯ï¼šé•¿åºåˆ—ï¼Œä½†éœ€è¦é¢‘ç¹è®¿é—®
   
   éœ€è¦åœ¨æ¯æ¬¡ Attention æ—¶ï¼š
   CPU â†’ GPU ä¼ è¾“ï¼ˆPCIe 3.0: ~16GB/sï¼‰

3ï¸âƒ£ NVMe SSD - å¤§è§„æ¨¡
   â”œâ”€ ä½ç½®ï¼šå›ºæ€ç¡¬ç›˜
   â”œâ”€ ä¼˜ç‚¹ï¼šå®¹é‡æœ€å¤§ï¼Œå¯å¤„ç†æé•¿åºåˆ—ï¼ˆ>1M tokensï¼‰
   â”œâ”€ ç¼ºç‚¹ï¼šè®¿é—®å»¶è¿Ÿæœ€é«˜ï¼ˆms çº§åˆ«ï¼‰
   â””â”€ ä½¿ç”¨åœºæ™¯ï¼šç¦»çº¿æ¨ç†ï¼Œè¶…é•¿åºåˆ—
   
   éœ€è¦é¢„å–å’Œå¼‚æ­¥ I/O æ¥éšè—å»¶è¿Ÿ
```

## 2. å¤§æ¨¡å‹ä¸­çš„ KV Cache å®ç°

### **vLLM çš„å®ç°ï¼ˆä¸šç•Œæ ‡å‡†ï¼‰**

vLLM æ˜¯ GPU æ¨ç†æ¡†æ¶ä¸­ KV Cache ç®¡ç†æœ€ä¼˜çš„å®ç°ï¼š

```python
# æ ¸å¿ƒæ€æƒ³ï¼šç‰©ç†å—ç®¡ç†ï¼ˆPhysical Block å’Œ Logical Blockï¼‰

# 1. ç‰©ç†å—åˆ†é…
class KVCacheManager:
    def __init__(self, num_gpu_blocks, block_size):
        self.gpu_blocks = GPUBlockAllocator(num_gpu_blocks, block_size)
        # block_size é€šå¸¸æ˜¯ 16 tokens
        # num_gpu_blocks æ ¹æ®æ˜¾å­˜è‡ªåŠ¨è®¡ç®—
    
    def allocate(self, seq_len):
        # åˆ†é…è¶³å¤Ÿçš„å—
        num_blocks = (seq_len + block_size - 1) // block_size
        blocks = self.gpu_blocks.allocate(num_blocks)
        return blocks

# 2. ç‰©ç†å—æ˜ å°„
# å¤šä¸ªè¯·æ±‚å¯ä»¥å…±äº«åŒä¸€å—ç‰©ç†å—ï¼ˆKV Cache å…±äº«ï¼‰
request1_kv = [block_1, block_2, block_3]  # æŒ‡å‘ç‰©ç†å—
request2_kv = [block_1, block_2, block_4]  # å‰ä¸¤å—å…±äº«ï¼

# 3. è®¿é—®æµç¨‹
for step in range(num_steps):
    # è·å–é€»è¾‘åœ°å€æ˜ å°„
    logical_blocks = request.kv_cache_blocks
    
    # è½¬æ¢åˆ°ç‰©ç†å—åœ°å€
    physical_blocks = mapping_table[logical_blocks]
    
    # GPU kernel ç›´æ¥æ“ä½œç‰©ç†å—
    attention_kernel(Q, physical_blocks, output)
    
    # ç”Ÿæˆæ–° token åï¼Œåˆ†é…æ–°å—
    new_block = allocate_block()
    request.kv_cache_blocks.append(new_block)
```

### **HuggingFace çš„å®ç°**

```python
# æ›´ç®€å•çš„å®ç°æ–¹å¼

class SimpleKVCache:
    def __init__(self, max_seq_len, hidden_dim):
        # é¢„å…ˆåˆ†é…å›ºå®šå¤§å°çš„å¼ é‡ï¼ˆæ˜¾å­˜ï¼‰
        self.key_cache = torch.zeros(
            (num_layers, max_seq_len, hidden_dim),
            device='cuda'
        )
        self.value_cache = torch.zeros(
            (num_layers, max_seq_len, hidden_dim),
            device='cuda'
        )
        self.cur_len = 0  # å½“å‰å¡«å……åˆ°çš„ä½ç½®
    
    def update(self, layer_idx, new_k, new_v):
        # æŠŠæ–°çš„ K, V è¿½åŠ åˆ°ç¼“å­˜
        self.key_cache[layer_idx, self.cur_len:self.cur_len+new_k.shape[0]] = new_k
        self.value_cache[layer_idx, self.cur_len:self.cur_len+new_v.shape[0]] = new_v
        self.cur_len += new_k.shape[0]
    
    def get(self, layer_idx):
        # è¿”å›å½“å‰æœ‰æ•ˆçš„ K, V
        return (
            self.key_cache[layer_idx, :self.cur_len],
            self.value_cache[layer_idx, :self.cur_len]
        )
```

### **æ•°æ®æµå‘**

```
æ¨ç†é˜¶æ®µï¼ˆæ¯æ­¥ï¼‰ï¼š

å¤–å­˜/CPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚
                   â†“
              é¢„å–ï¼ˆå¯é€‰ï¼‰
                   â”‚
                   â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  æ˜¾å­˜ç¼“å†²åŒº  â”‚  â† æ–° token å¯¹åº”çš„ K, V
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   GPU æ˜¾å­˜       â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  KV Cache åŒºåŸŸ   â”‚  â† å†å² K, Vï¼ˆæ ¸å¿ƒï¼ï¼‰
         â”‚  (150-300MB)     â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  æ¨¡å‹å‚æ•°        â”‚
         â”‚  (70B: 140GB)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â†“
            Attention è®¡ç®—
                   â”‚
                   â†“
              ç”Ÿæˆä¸‹ä¸€ token
                   â”‚
                   â””â”€â”€â†’ ä¿å­˜åˆ° KV Cache
```

## 3. KV Cache æ˜¾å­˜å ç”¨åˆ†æ

### **å…¬å¼æ¨å¯¼**

```
KV Cache æ˜¾å­˜ = æ¯ä¸ª KV å¯¹å ç”¨

åŸºç¡€è®¡ç®—ï¼š
  æ¯ä¸ªä½ç½®çš„ K æˆ– V = hidden_dim Ã— dtype_size
  
  1 ä¸ª token çš„ 1 å±‚ KV = 2 Ã— hidden_dim Ã— dtype_size
  
  å®Œæ•´ KV Cache = 2 Ã— seq_len Ã— hidden_dim Ã— num_layers Ã— dtype_size

å®é™…ä¾‹å­ï¼ˆLLaMA 7B æ¨¡å‹ï¼‰ï¼š
  - hidden_dim = 4096
  - num_layers = 32
  - dtype = float16 (2 bytes)
  - seq_len = 2048
  
  KV_cache = 2 Ã— 2048 Ã— 4096 Ã— 32 Ã— 2
           â‰ˆ 1GB per request
```

### **å¤šè¯·æ±‚åœºæ™¯ä¸‹çš„èŠ‚çœ**

```
N ä¸ªè¯·æ±‚å…±äº«åŒä¸€ User History çš„ KV Cacheï¼š

ä¼ ç»Ÿæ–¹å¼ï¼š
  æ€»æ˜¾å­˜ = N Ã— (KV_user + KV_item)
  
ä¼˜åŒ–æ–¹å¼ï¼š
  æ€»æ˜¾å­˜ = KV_user + N Ã— KV_item
  
èŠ‚çœé‡ï¼ˆLONGER çš„å…³é”®ï¼‰ï¼š
  èŠ‚çœ = (N-1) Ã— KV_user
  
ç¤ºä¾‹ï¼ˆ50 ä¸ªå€™é€‰ itemsï¼‰ï¼š
  KV_user â‰ˆ 100MB
  èŠ‚çœ = 49 Ã— 100MB â‰ˆ 4.9GB âœ…
```

## 4. å…³é”®æŒ‡æ ‡

### **æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”**

| æŒ‡æ ‡ | æ—  KV Cache | æœ‰ KV Cache | æ”¹è¿› |
|------|-----------|----------|------|
| **å†…å­˜å ç”¨** | å¾ˆå¤§ âŒ | å° âœ… | -60% |
| **è®¡ç®—é‡** | O(nÂ²) | O(n) | çº¿æ€§ âœ… |
| **æ¨ç†é€Ÿåº¦** | æ…¢ | å¿« âœ… | 5-10x |
| **å»¶è¿Ÿ** | ~1s | ~10ms | 100x âœ… |

### **æ¨èæœ€ä½³å®è·µ**

```
âœ… ä½¿ç”¨ KV Cache çš„åœºæ™¯ï¼š
  â”œâ”€ æ–‡æœ¬ç”Ÿæˆï¼ˆLLMï¼‰
  â”œâ”€ ç¿»è¯‘
  â”œâ”€ æ¨èæ’åºï¼ˆLONGERï¼‰
  â””â”€ å®æ—¶å¯¹è¯

âŒ ä¸éœ€è¦ KV Cache çš„åœºæ™¯ï¼š
  â”œâ”€ åˆ†ç±»ä»»åŠ¡
  â”œâ”€ ä¸€æ¬¡æ€§æ¨ç†
  â””â”€ ç¦»çº¿æ‰¹å¤„ç†
```

## å‚è€ƒèµ„æº

è¯¦ç»†çš„å·¥ç¨‹å®ç°æŒ‡å—è§ï¼š[kvcache_engineering.md](./kvcache_engineering.md)

ç›¸å…³æŠ€æœ¯æ–‡ç« ï¼š
- vLLMï¼šhttps://github.com/lm-sys/vllm
- FlashAttentionï¼šhttps://github.com/HazyResearch/flash-attention
- HuggingFace Transformersï¼šhttps://huggingface.co/docs/transformers/
